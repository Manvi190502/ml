{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxW7HpyjXrB+XrmSgMFzjT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manvi190502/ml/blob/main/Pima_indian_diabetes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxfRD7I6Bd8D",
        "outputId": "a61bba90-3cce-435a-8012-4b32942438ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  6.    148.     72.    ...  33.6     0.627  50.   ]\n",
            " [  1.     85.     66.    ...  26.6     0.351  31.   ]\n",
            " [  8.    183.     64.    ...  23.3     0.672  32.   ]\n",
            " ...\n",
            " [  5.    121.     72.    ...  26.2     0.245  30.   ]\n",
            " [  1.    126.     60.    ...  30.1     0.349  47.   ]\n",
            " [  1.     93.     70.    ...  30.4     0.315  23.   ]]\n",
            "Epoch 1/50\n",
            "77/77 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.5651\n",
            "Epoch 2/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.9767 - accuracy: 0.5573\n",
            "Epoch 3/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.8286 - accuracy: 0.5794\n",
            "Epoch 4/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7764 - accuracy: 0.6198\n",
            "Epoch 5/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7459 - accuracy: 0.6120\n",
            "Epoch 6/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7250 - accuracy: 0.6315\n",
            "Epoch 7/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7148 - accuracy: 0.6289\n",
            "Epoch 8/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6773 - accuracy: 0.6484\n",
            "Epoch 9/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6837 - accuracy: 0.6471\n",
            "Epoch 10/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6984 - accuracy: 0.6315\n",
            "Epoch 11/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6443 - accuracy: 0.6484\n",
            "Epoch 12/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6601 - accuracy: 0.6536\n",
            "Epoch 13/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6526 - accuracy: 0.6562\n",
            "Epoch 14/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6349 - accuracy: 0.6667\n",
            "Epoch 15/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6436 - accuracy: 0.6536\n",
            "Epoch 16/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6354 - accuracy: 0.6549\n",
            "Epoch 17/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6188 - accuracy: 0.6758\n",
            "Epoch 18/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6244 - accuracy: 0.6784\n",
            "Epoch 19/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6192 - accuracy: 0.6784\n",
            "Epoch 20/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6205 - accuracy: 0.6758\n",
            "Epoch 21/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6282 - accuracy: 0.6758\n",
            "Epoch 22/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6138 - accuracy: 0.6667\n",
            "Epoch 23/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6093 - accuracy: 0.6901\n",
            "Epoch 24/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6015 - accuracy: 0.6901\n",
            "Epoch 25/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6210 - accuracy: 0.6810\n",
            "Epoch 26/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6021 - accuracy: 0.7057\n",
            "Epoch 27/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6129 - accuracy: 0.6914\n",
            "Epoch 28/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5934 - accuracy: 0.6810\n",
            "Epoch 29/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5880 - accuracy: 0.7109\n",
            "Epoch 30/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5951 - accuracy: 0.7044\n",
            "Epoch 31/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5864 - accuracy: 0.6888\n",
            "Epoch 32/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5853 - accuracy: 0.6888\n",
            "Epoch 33/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5876 - accuracy: 0.6849\n",
            "Epoch 34/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5913 - accuracy: 0.7018\n",
            "Epoch 35/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5722 - accuracy: 0.7122\n",
            "Epoch 36/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5706 - accuracy: 0.7188\n",
            "Epoch 37/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5830 - accuracy: 0.7083\n",
            "Epoch 38/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5699 - accuracy: 0.7031\n",
            "Epoch 39/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5647 - accuracy: 0.7096\n",
            "Epoch 40/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5668 - accuracy: 0.7057\n",
            "Epoch 41/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5602 - accuracy: 0.7148\n",
            "Epoch 42/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5643 - accuracy: 0.7266\n",
            "Epoch 43/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5595 - accuracy: 0.7188\n",
            "Epoch 44/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5490 - accuracy: 0.7331\n",
            "Epoch 45/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5525 - accuracy: 0.7227\n",
            "Epoch 46/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5568 - accuracy: 0.7122\n",
            "Epoch 47/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5553 - accuracy: 0.7188\n",
            "Epoch 48/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5481 - accuracy: 0.7344\n",
            "Epoch 49/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5397 - accuracy: 0.7227\n",
            "Epoch 50/50\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5362 - accuracy: 0.7552\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5306 - accuracy: 0.7422\n",
            "Accuracy: 74.22\n",
            "Saved model to disk\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "   1. Number of times pregnant\n",
        "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
        "   3. Diastolic blood pressure (mm Hg)\n",
        "   4. Triceps skin fold thickness (mm)\n",
        "   5. 2-Hour serum insulin (mu U/ml)\n",
        "   6. Body mass index (weight in kg/(height in m)^2)\n",
        "   7. Diabetes pedigree function\n",
        "   8. Age (years)\n",
        "   9. Class variable (0 or 1)\n",
        "'''\n",
        "\n",
        "\n",
        "from numpy import loadtxt #load the dataset\n",
        "from keras.models import Sequential #adding the layers in sequential order\n",
        "from keras.layers import Dense #mathematical\n",
        "\n",
        "#load the data into the dataset variable\n",
        "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',') #read/load the dataset\n",
        "\n",
        "#segregate dataset into input and output\n",
        "x = dataset[:,0:8] #input/feature/x\n",
        "y = dataset[:,8] #output/classes/y\n",
        "print(x)\n",
        "\n",
        "#designing neural neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#compile\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#train\n",
        "model.fit(x, y, epochs=50, batch_size=10)\n",
        "\n",
        "#evaluate the model\n",
        "_, accuracy = model.evaluate(x, y)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "import numpy as np\n",
        "from numpy import loadtxt\n",
        "from keras.models import model_from_json\n",
        "\n",
        "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
        "x = dataset[:,0:8]\n",
        "y = dataset[:,8]\n",
        "\n",
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "\n",
        "model = model_from_json(loaded_model_json)\n",
        "model.load_weights(\"model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "#predictions = model.predict_classes(x)\n",
        "predict_x=model.predict(x)\n",
        "classes_x=np.argmax(predict_x,axis=1)\n",
        "\n",
        "for i in range(5,10):\n",
        "\tprint('%s => %d (Original Class: %d)' % (x[i].tolist(), classes_x[i], y[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rZbqi0zB2sY",
        "outputId": "b2845bd0-d5aa-4208-ad2d-bc893042b40a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from disk\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "[5.0, 116.0, 74.0, 0.0, 0.0, 25.6, 0.201, 30.0] => 0 (Original Class: 0)\n",
            "[3.0, 78.0, 50.0, 32.0, 88.0, 31.0, 0.248, 26.0] => 0 (Original Class: 1)\n",
            "[10.0, 115.0, 0.0, 0.0, 0.0, 35.3, 0.134, 29.0] => 0 (Original Class: 0)\n",
            "[2.0, 197.0, 70.0, 45.0, 543.0, 30.5, 0.158, 53.0] => 0 (Original Class: 1)\n",
            "[8.0, 125.0, 96.0, 0.0, 0.0, 0.0, 0.232, 54.0] => 0 (Original Class: 1)\n"
          ]
        }
      ]
    }
  ]
}